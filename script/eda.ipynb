{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridefare Dynamics and Predictive Pricing\n",
    "\n",
    "**Name(s)**: Ken Fukutomi\n",
    "\n",
    "**Website Link**: (your website link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install walkscore_api\n",
    "!pip install folium\n",
    "!pip install statistics\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import statistics\n",
    "\n",
    "# Misc:\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "from lec_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nything that might be useful.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my urban analytics related components\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from walkscore import WalkScoreAPI\n",
    "import folium\n",
    "from folium.plugins import FastMarkerCluster\n",
    "\n",
    "## For reading in my environment key\n",
    "load_dotenv(find_dotenv())\n",
    "assert(os.getenv(\"WALKSCORE_API\"))\n",
    "key = (os.getenv('WALKSCORE_API'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in my dataset, from Kaggle.\n",
    "df = pd.read_csv(\"riders.csv\")\n",
    "col_list = df.columns.unique().tolist()\n",
    "assert(len(col_list) == df.shape[1])\n",
    "\n",
    "print(df.shape[0], ',', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I begin to explore this dataset, several questions come to mind that would be useful in a real‑world context:\n",
    "\n",
    "- During peak hours, should I choose Uber or Lyft to minimize my fare? \n",
    "- How does surge pricing vary by time of day and by service?\n",
    "- What is the relationship between trip distance and price for each cab type?\n",
    "- Do weather conditions (e.g., temperature, uv-index) affect average fares in BOS? \n",
    "- Which neighborhoods tend to incur higher or lower ride costs?\n",
    "- What are some of the trip hotspots in Boston? Do they originate from areas w/ a younger audience? Working people?\n",
    "\n",
    "Specifically, for the predictive portion of this project, I will first establish a baseline model and then refine it into a final, improved model to better understand and predict trip fares based on various factors in the dataset.\n",
    "\n",
    "**Dataset overview:**  \n",
    "- **Number of rows:** 693,071 \n",
    "- **Relevant columns:**  \n",
    "  - `timestamp` / `hour` (ride timing)  \n",
    "  - `cab_type` (Uber vs. Lyft)  \n",
    "  - `price` (fare cost)  \n",
    "  - `surge_multiplier` (demand pricing)  \n",
    "  - `distance` (trip length)  \n",
    "  - `source` / `destination` (pickup -> trip destination)\n",
    "  - `product_id` Type of Ridership Service (Shared, XL, Comfort, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis\n",
    "\n",
    "Cleaning:\n",
    "1. Probabilistic Imputation / Distance-Based Imputation for NaN\n",
    "2. Standardize Uber/Lyft Type, ReGex to Convert to ASCII\n",
    "3. Combine Metric(s) into One, Simplify\n",
    "4. Standardize Datetime Column\n",
    "5. Drop duplicate and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what some variables we're missing, NaN\n",
    "## Cleaning 1\n",
    "\n",
    "# 1. Summary of Missing Values.\n",
    "missing_counts = df.isna().sum()\n",
    "missing_counts = (\n",
    "    missing_counts[missing_counts > 0]\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nan_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_counts.astype(int),\n",
    "    '% NaN': (missing_counts / len(df) * 100).round(2)\n",
    "})\n",
    "print(nan_summary, '\\n')\n",
    "nulls = df[df['price'].isna()]\n",
    "target = ('distance') #let's save for now.\n",
    "\n",
    "## Imputing for Prices.\n",
    "# 2. Conditionally impute:\n",
    "def impute_prob(df: pd.Series) -> pd.Series:\n",
    "    local = df.copy()\n",
    "    num_missing = local.isna().sum()\n",
    "    sample = np.random.choice(local.dropna(), num_missing)\n",
    "    local.loc[local.isna()] = sample\n",
    "    return local\n",
    "\n",
    "prob_imputed = impute_prob(df['price'])\n",
    "##print(f\"Pre-Imputation Mean: {df['price'].mean()}\")\n",
    "print(f\"Computed Probabilistic Mean: ${prob_imputed.mean():.2f}\")\n",
    "\n",
    "# 3. Let's try to impute based on distance:\n",
    "df['dist_bin'] = pd.qcut(df['distance'], q=5, duplicates='drop')\n",
    "#print(f\"{df['dist_bin']}\")\n",
    "median_by_qbin = (\n",
    "    df\n",
    "    .groupby('dist_bin', observed=False)\n",
    "    ['price']\n",
    "    .transform('median')\n",
    ")\n",
    "\n",
    "distance_imputed = (df['price'].fillna(median_by_qbin))\n",
    "mean_distance = distance_imputed.mean()\n",
    "print(f'Distance-Based Imputed Mean: ${mean_distance:.2f}')\n",
    "diff = abs(mean_distance - prob_imputed.mean())\n",
    "print(f\"Absolute difference: ${diff:.2f}\")\n",
    "\n",
    "\n",
    "# imputation apply,\n",
    "df['price'] = prob_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    'Original': df['price'].dropna(),\n",
    "    'Probabilistic Impute': prob_imputed,\n",
    "    'Distance-Based Impute': distance_imputed\n",
    "})\n",
    "\n",
    "df_melt = (\n",
    "    df_plot\n",
    "    .melt(var_name='Method', value_name='price')\n",
    ")\n",
    "fig = px.histogram(\n",
    "    df_melt,\n",
    "    x='price',\n",
    "    color='Method',\n",
    "    histnorm='density',\n",
    "    nbins=40,\n",
    "    barmode='overlay',\n",
    "    opacity=0.5,\n",
    "    title='Fare Price Distribution: Pre vs Post-Imputation'\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html(\"assets/fare-imputation-comparison.html\", include_plotlyjs=\"cdn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the two imputation methods yield nearly identical distributions and the overall percentage of missing values is low, we can safely conclude that the choice of imputation strategy will not materially affect our analysis. Therefore, we adopt a probabilistic imputation approach, sampling each missing value from the empirical distribution of observed values:\n",
    "\n",
    "$$\n",
    "x_i^{(\\mathrm{imputed})} = x_j,\\quad\n",
    "j \\sim \\mathrm{Uniform}\\bigl(\\{k \\mid x_k \\neq \\mathrm{NaN}\\}\\bigr).\n",
    "$$\n",
    "\n",
    "Equivalently, each observed value has **equal probability**:\n",
    "\n",
    "$$\n",
    "P\\bigl(x_i^{(\\mathrm{imputed})} = x_j\\bigr)\n",
    "= \\frac{1}{n_{\\mathrm{observed}}},\n",
    "$$\n",
    "\n",
    "Regardless, my selection for which method doesn't affect the overall distribution. Hence, I could've just kept variabales as NaN, but I preferred to have it imputed, in order to have a full column of information (pricing, as it becomes relevant later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning 2, Expanding/Closing 'data'\n",
    "# Cleaning Product ID Column\n",
    "\n",
    "def normalize_product_ids(df, col='product_id'):\n",
    "\n",
    "    # UUID regex pattern\n",
    "    uuid_re = re.compile(\n",
    "        r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # UUID --> lyft_* mapping\n",
    "    uuid_to_name = {\n",
    "        '6f72dfc5-27f1-42e8-84db-ccc7a75f6969': 'lyft_premier',\n",
    "        '9a0e7b09-b92b-4c41-9779-2ad22b4d779d': 'lyft',\n",
    "        '6d318bcc-22a3-4af6-bddd-b409bfce1546': 'lyft_luxsuv',\n",
    "        '6c84fd89-3f11-4782-9b50-97c468b19529': 'lyft_plus',\n",
    "        '8cf7e821-f0d3-49c6-8eba-e679c0ebcf6a': 'lyft_lux',\n",
    "        '55c66225-fbe7-4fd5-9072-eab1ece5e23e': 'lyft_line',\n",
    "        '997acbb5-e102-41e1-b155-9df7de0a73f2': 'lyft_shared'\n",
    "    }\n",
    "\n",
    "    df['product_id_clean'] = df[col].astype(str).str.strip().str.lower()\n",
    "    mask = (~df['product_id_clean'].str.match(uuid_re))\n",
    "    problematic = (\n",
    "        df\n",
    "        .loc[mask, 'product_id_clean']\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    res_init = (df[col].unique().tolist())\n",
    "    print(\"Before replacement:\\n\", res_init, '\\n')\n",
    "\n",
    "    # Output diffs:\n",
    "    df['product_id_clean'] = df['product_id_clean'].replace(uuid_to_name)\n",
    "\n",
    "    res = df['product_id_clean'].unique().tolist()\n",
    "    print(\"After cleaning:\\n\", res, '\\n')\n",
    "    return df\n",
    "\n",
    "df = normalize_product_ids(df)\n",
    "df.drop(columns='product_id', inplace=True)\n",
    "df.rename(columns={'product_id_clean' : 'product_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean DF\n",
    "# Aggregate total price and ride count per product and name\n",
    "agg2 = (\n",
    "    df.groupby(['product_id', 'name'])['price']\n",
    "    .agg(total_price='sum', ride_count='count')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "display(agg2)\n",
    "\n",
    "# name, product_id are the same.\n",
    "df = df.drop(['product_id'],axis=1)\n",
    "clean_r=df['name'].unique().tolist()\n",
    "print(f'Final Cleaning of product_id: {clean_r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning 2, Expanding/Closing 'data'\n",
    "# Continued, Feature Engineering\n",
    "\n",
    "# repeat column, convert to datetime format.\n",
    "df.drop(columns=['visibility.1'], inplace=True)\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "assert isinstance(df['datetime'].iloc[0], dt.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a common departure time column based on source\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "most_common_hour = (\n",
    "    df.groupby('source')['hour']\n",
    "      .agg(lambda x: x.mode().iloc[0])\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "df['most_common'] = df['source'].map(most_common_hour)\n",
    "#df['most_common'].unique().tolist()\n",
    "\n",
    "## Applying more, let's compute a duration col, given the distance.\n",
    "assert df['distance'].shape[0] == df.shape[0]\n",
    "\n",
    "# estimated_duration = (distance_miles / avg_speed) * 60\n",
    "# above creates multicolinearity w/ distance\n",
    "\n",
    "# remove cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'id', 'timezone', 'short_summary', 'long_summary',\n",
    "    'windGustTime', 'temperatureHighTime', 'temperatureLowTime',\n",
    "    'temperatureHigh', 'temperatureLow', 'humidity',\n",
    "    'dewPoint', 'uvIndex', 'ozone', 'moonPhase',\n",
    "    'temperatureMinTime', 'apparentTemperatureMax',\n",
    "    'apparentTemperatureMaxTime', 'apparentTemperatureMin',\n",
    "    'apparentTemperatureMinTime'\n",
    "])\n",
    "\n",
    "# above are all low-correlation figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['hour', 'distance', 'price', 'cab_type', 'name', 'destination']\n",
    "print(df[relevant_columns].head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Boston Driving Speed: https://www.cbsnews.com/boston/news/boston-traffic-study-2024/#:~:text=INRIX%20found%20that%20the%20average,second%2Dslowest%20in%20the%20country.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Univariate Mapping/Visualization\n",
    "# VISUALIZATION 1: Trip Distance Distribution\n",
    "\n",
    "fig = px.histogram(\n",
    "    df,\n",
    "    x='distance',\n",
    "    nbins=50,\n",
    "    title='Trip Distance Distribution',\n",
    "    labels={'distance':'Distance (in miles)'},\n",
    "    #log_y=True,\n",
    "    color_discrete_sequence=['indianred'],\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Distance (in miles)',\n",
    "    yaxis_title='Frequency',\n",
    "    bargap=0.05\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html(\"assets/distance.html\", include_plotlyjs=\"cdn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(df, col:str):\n",
    "    # plot w/ logarithmic transformation\n",
    "    key=(col+'_log')\n",
    "    df[key] = np.log1p(df[col])\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=key,\n",
    "        nbins=50,\n",
    "        title='Log-Transformed Trip Distance Distribution',\n",
    "        labels={key: f'Log({col} + 1)'}\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=f'Log({col} + 1)',\n",
    "        yaxis_title='Frequency',\n",
    "        bargap=0.05\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fig2 = log_transform(df, 'distance')\n",
    "fig2.show()\n",
    "fig2.write_html(\"assets/log.html\", include_plotlyjs=\"cdn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Univariate Mapping/Visualization\n",
    "# VISUALIZATION 2: Count of Trip per Hour\n",
    "\n",
    "trip_cn = (\n",
    "    df['hour']\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ") #trip_cn\n",
    "\n",
    "trip_cn.columns = ['hour', 'trip_count']\n",
    "\n",
    "fig = px.bar(\n",
    "    trip_cn,\n",
    "    x='hour',\n",
    "    y='trip_count',\n",
    "    title='Trip Counts per Hour of Day',\n",
    "    labels={\n",
    "        'hour': 'Hour of the Day',\n",
    "        'trip_count': 'Frequency of Trip(s)'\n",
    "    },\n",
    "    color_discrete_sequence=['purple'],\n",
    "    log_y=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(dtick=1),\n",
    "    title_font_size=15,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, trips do tend to be all over the place, but it's evident that past Midnight until 8, we see a declining of total routed trips across Boston. It'd be more clever to look at the distribution of departed trip for every POI (place(s) of interest), accounting for other factors such as connectivity to public transit access, or even looking at the walkability of the given area to another destination. Often times, people might be less incentivized to take shorter trips, leading them to take alternative approaches. Hence why I also self computed for a **duration** column given the distance(s) of trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Univariate Mapping/Visualization\n",
    "# VISUALIZATION 3: Trip Frequency by Random Origin\n",
    "\n",
    "x = df['source'].unique().tolist()\n",
    "loc = np.random.choice(x)\n",
    "subset = df[df['source'] == loc]\n",
    "\n",
    "# build my base map\n",
    "center_lat = subset['latitude'].mean()\n",
    "center_lon = subset['longitude'].mean()\n",
    "trip_map = folium.Map(\n",
    "    location=[center_lat, center_lon], \n",
    "    zoom_start=14,\n",
    "    tiles='CartoDB Dark_Matter',\n",
    ")\n",
    "\n",
    "# cluster markers\n",
    "coors = (\n",
    "    subset[[\n",
    "        'latitude',\n",
    "        'longitude'\n",
    "    ]]\n",
    "    .dropna()\n",
    "    .values.tolist()\n",
    ")\n",
    "FastMarkerCluster(coors).add_to(trip_map)\n",
    "\n",
    "trip_map\n",
    "trip_map.save(\"assets/trip-frequency-map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis / Interesting Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bivariate Mapping/Visualization\n",
    "# VISUALIZATION 1: Fare Prices by Cab Type (Uber, Lyft)\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    y='cab_type',\n",
    "    x='price',\n",
    "    color='cab_type',\n",
    "    color_discrete_sequence=['blue', 'orange'],\n",
    "    title='Fare Price by Cab Type (Uber | Lyft)',\n",
    "    labels={'cab_type': 'Cab Type', 'price': 'Price/Fare ($ USD)'},\n",
    "    log_x=False\n",
    ")\n",
    "\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bivariate Mapping/Visualization\n",
    "# VISUALIZATION 2: Heatmap correlation of features\n",
    "\n",
    "numeric_cols = [\n",
    "    'hour', 'day', 'month', 'price', 'distance', 'surge_multiplier',\n",
    "    'latitude', 'longitude', 'temperature', 'apparentTemperature',\n",
    "    'precipIntensity', 'precipProbability', 'windSpeed', 'windGust',\n",
    "    'visibility', 'apparentTemperatureHigh', 'apparentTemperatureLow',\n",
    "    'pressure', 'windBearing', 'cloudCover', 'precipIntensityMax',\n",
    "    'temperatureMin', 'temperatureMax', 'distance_log'\n",
    "]\n",
    "\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto=True,\n",
    "    aspect=\"auto\",\n",
    "    title=\"Feature Correlation Heatmap\",\n",
    "    color_continuous_scale=\"Viridis\"\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html(\"assets/corr.html\", include_plotlyjs=\"cdn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interesting Aggregations:\n",
    "agg_split = (\n",
    "    df.groupby(['destination', 'cab_type'])[['price', 'distance']]\n",
    "      .agg(['mean', 'median'])\n",
    "      .round(2)\n",
    "      .sort_values(('price', 'median'), ascending=False)\n",
    ")\n",
    "\n",
    "agg_split.columns = ['_'.join(col) for col in agg_split.columns]\n",
    "agg_split = agg_split.reset_index()\n",
    "\n",
    "uber = agg_split[agg_split['cab_type'] == 'Uber']\n",
    "lyft = agg_split[agg_split['cab_type'] == 'Lyft']\n",
    "uber.sort_values('price_median', ascending=False)\n",
    "lyft.sort_values('price_median', ascending=False)\n",
    "display(lyft)\n",
    "display(uber)\n",
    "\n",
    "## Bivariate Mapping #3\n",
    "fig = px.bar(\n",
    "    agg_split,\n",
    "    x='destination',\n",
    "    y='price_median',\n",
    "    color='cab_type',\n",
    "    barmode='group',\n",
    "    text='price_median',\n",
    "    facet_col='cab_type',\n",
    "    title='Median Trip Price by Source and Cab Type',\n",
    "    labels={'price_median': 'Median Price ($)','destination': 'Pickup Area'},\n",
    ")\n",
    "fig.update_layout(showlegend=False, bargap=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkscore(df: pd.DataFrame, df2) -> list:\n",
    "    import reverse_geocode\n",
    "    # out of interest:\n",
    "    ls1 = []\n",
    "    ls2 = []\n",
    "\n",
    "    def __init__(df, ls1, ls2):\n",
    "        ls1 = (\n",
    "            df\n",
    "            .sort_values('price_mean',ascending=False)\n",
    "            .head(3)['destination']\n",
    "            .tolist()\n",
    "        )\n",
    "        ls2 = (\n",
    "            df\n",
    "            .sort_values('price_mean',ascending=True)\n",
    "            .head(3)['destination']\n",
    "            .tolist()\n",
    "        )\n",
    "        return ls1, ls2\n",
    "    \n",
    "    tups=(__init__(df, ls1, ls2))\n",
    "    # tups[0], top\n",
    "    # tups[1], bottom\n",
    "\n",
    "    def compute_scores(tups, df2, key):\n",
    "\n",
    "        walkscore_api = WalkScoreAPI(api_key=key)\n",
    "        scores = {}\n",
    "\n",
    "        for i in range(2): # 0 = top 3, 1 = bottom 3\n",
    "            bike_scores = []\n",
    "            walk_scores = []\n",
    "\n",
    "            for location in tups[i]:\n",
    "                match = df2[df2['destination'] == location][['latitude', 'longitude']].head(1)\n",
    "                if not match.empty:\n",
    "                    lat, lon = match.iloc[0]\n",
    "                    result = walkscore_api.get_score(latitude=lat, longitude=lon)\n",
    "\n",
    "                    bike_scores.append(int(result.bike_score))\n",
    "                    walk_scores.append(int(result.walk_score))\n",
    "\n",
    "            scores[i] = {\n",
    "                'bikescore': bike_scores,\n",
    "                'walkscore': walk_scores\n",
    "            }\n",
    "        return scores\n",
    "    \n",
    "    return(compute_scores(tups, df2, key))\n",
    "        \n",
    "computed_tuple = compute_walkscore(uber, df)\n",
    "print('Connectivity Metric Median for Top-3 Destination')\n",
    "print('Bike',statistics.mean(computed_tuple[0]['bikescore']))\n",
    "print('Walk',statistics.mean(computed_tuple[0]['walkscore']), '\\n')\n",
    "print('Connectivity Metric Median for Bottom-3 Destination')\n",
    "print('Bike',statistics.mean(computed_tuple[1]['bikescore']))\n",
    "print('Walk',statistics.mean(computed_tuple[1]['walkscore']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noteworthy that transit access of an area might be a factor. However, it's also note-worthy that a lot of the top locations are nearby a young crowd (universities, downtown, areas with many clubs, bars); which might be a factor for people to take alternative shared-use mobility systems such as these Uber/Lyft trips. A higher income bracket would definitely correlate with this behavior as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Framing a Prediction Problem\n",
    "\n",
    "I'm tackling a regression task: predicting the ride fare price (`price`) for each trip in Boston.  \n",
    "\n",
    "- Response variable: `price`—this is the value riders see and platforms optimize for, and it must be known at booking time.  \n",
    "- Features used: only data available when the ride is requested—trip distance, pickup hour/day/month, surge multiplier, cab type & service tier, origin & destination neighborhoods, and forecasted weather metrics. No in-ride or post-trip information is included to avoid leakage.  \n",
    "- Evaluation metric: RMSE as our primary score (to heavily penalize large dollar‐value mistakes) and mean absolute error for a straightforward average-error interpretation in dollars.  \n",
    "\n",
    "By restricting inputs and using RMSE/MAE, I can ensure that my model mimics real-world fare estimation and prioritizes minimizing costly prediction outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw a 10% random sample for quicker iteration\n",
    "df_sampled = df.sample(frac=0.15, random_state=42)\n",
    "print(df_sampled.shape[0])\n",
    "\n",
    "# Base Line Model;\n",
    "## TECHNIQUE: Multiple Linear Regression with Scaling & One-Hot Encoding\n",
    "class BaselineModel:\n",
    "    def __init__(self, df, test_size=0.2, random_state=42):\n",
    "        self.df = df\n",
    "        self.FEATURES = ['distance', 'hour', 'cab_type']\n",
    "        self.TARGET = 'price'\n",
    "        X = df[self.FEATURES]\n",
    "        y = df[self.TARGET]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        num_feats = ['distance', 'hour']\n",
    "        cat_feats = ['cab_type']\n",
    "        num_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler',   StandardScaler())\n",
    "        ])\n",
    "        cat_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', num_pipe, num_feats),\n",
    "            ('cat', cat_pipe, cat_feats)\n",
    "        ], sparse_threshold=0)\n",
    "        self.pipeline = Pipeline([\n",
    "            ('preproc', preprocessor),\n",
    "            ('model', LinearRegression())\n",
    "        ])\n",
    "\n",
    "    def fit(self):\n",
    "        self.pipeline.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def evaluate(self):\n",
    "        preds = self.pipeline.predict(self.X_test)\n",
    "        print(f\"Baseline RMSE:{mean_squared_error(self.y_test,preds,squared=False):.2f}\")\n",
    "        print(f\"Baseline MAE :{mean_absolute_error(self.y_test,preds):.2f}\")\n",
    "        print(f\"Baseline R^2 :{r2_score(self.y_test,preds):.3f}\")\n",
    "\n",
    "# usage, end\n",
    "# utilize our local sampled_df size\n",
    "# performance comparable to full-size df of 60k rows\n",
    "model = BaselineModel(df_sampled)\n",
    "model.fit()\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering 1\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['is_weekend'] = (df['timestamp'].dt.weekday >= 5).astype(int)\n",
    "\n",
    "df_sampled['timestamp'] = pd.to_datetime(df_sampled['timestamp'])\n",
    "df_sampled['is_weekend'] = (df_sampled['timestamp'].dt.weekday >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering 2\n",
    "# LOOK AT TRIP ORIGIN\n",
    "#df['source'].unique().tolist()\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df['source'], prefix='source', dtype=int)], axis=1)\n",
    "df_sampled = pd.concat([df_sampled, pd.get_dummies(df_sampled['source'], prefix='source', dtype=int)], axis=1)\n",
    "\n",
    "# Leads to multicolinearity (?) --> Q to self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MORE feature engineering!\n",
    "# understanding peak hours:\n",
    "peak_hours = [7, 8, 9, 16, 17, 18]\n",
    "df['is_peak_hour'] = df['hour'].isin(peak_hours).astype(int)\n",
    "df_sampled['is_peak_hour'] = df_sampled['hour'].isin(peak_hours).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop pre expanded one hots e.g., source)*\n",
    "cols = ['timestamp','datetime','icon','sunriseTime','sunsetTime',\n",
    "        'apparentTemperatureHigh','apparentTemperatureHighTime','apparentTemperatureLow',\n",
    "        'apparentTemperatureLowTime','precipIntensityMax','uvIndexTime','temperatureMaxTime',\n",
    "        'dist_bin','most_common'\n",
    "]\n",
    "\n",
    "df.drop(cols, axis=1, errors=\"ignore\", inplace=True)\n",
    "# columns after pruning are above in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up w dask distributed\n",
    "# new imports \n",
    "import dask.dataframe as dd \n",
    "from xgboost import dask as dxgb \n",
    "from dask.distributed import Client\n",
    "from xgboost import XGBRegressor, callback\n",
    "\n",
    "# start a local dask cluster \n",
    "#client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\n",
    "    'distance', 'hour', 'day', 'month',\n",
    "    'cab_type', 'source', 'destination',\n",
    "    'is_weekend', 'is_peak_hour'\n",
    "]\n",
    "\n",
    "df2 = df[feats].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.head())\n",
    "print(df2.dtypes)\n",
    "print(df2.columns)\n",
    "\n",
    "# there are categorical object variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omit certain columns with less importance, for the general observation made so far, as well as for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply importance of geospatial information for temporal pattern\n",
    "# cols of location : source, destination, one hot encoded source_*\n",
    "\n",
    "# --> the information in regards to exact start/end address is limited\n",
    "# factors that impact ride(s) -> precipIntensity, cab_type, distance\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external py --? run evals.py w/ provided data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
